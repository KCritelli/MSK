{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 136444\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "FLAGS = None\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 10\n",
    "EMBEDDING_SIZE = 50\n",
    "n_words = 0\n",
    "MAX_LABEL = 15\n",
    "WORDS_FEATURE = 'words'\n",
    "# Prepare training and testing data  \n",
    "cancerTotal = pd.merge(cancer_data, cancer_text, on='ID', how='inner')\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancerTotal.Text, cancerTotal.Class, test_size=0.2, random_state=42)\n",
    "\n",
    "  \n",
    "# Process vocabulary  \n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(len(cancerTotal.Text))  \n",
    "X_train = np.array(list(vocab_processor.fit_transform(X_train))) \n",
    "X_test = np.array(list(vocab_processor.transform(X_test)))\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' % n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_model(features, target):   \n",
    "    target = tf.one_hot(target, 9, 1, 0)  \n",
    "    features = tf.contrib.layers.bow_encoder(features, vocab_size=n_words, embed_dim=EMBEDDING_SIZE)  \n",
    "    logits = tf.contrib.layers.fully_connected(features, 9,activation_fn=None)  \n",
    "    loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n",
    "    train_op = tf.contrib.layers.optimize_loss(loss, tf.contrib.framework.get_global_step(),optimizer='Adam', learning_rate=0.01)  \n",
    "    return ({'class': tf.argmax(logits, 1), 'prob': tf.nn.softmax(logits)},loss, train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/vz/p2k6whv90_91t9567mpw6x1h0000gn/T/tmpr3j1ipyp\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11be6af98>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/var/folders/vz/p2k6whv90_91t9567mpw6x1h0000gn/T/tmpr3j1ipyp'}\n",
      "WARNING:tensorflow:From <ipython-input-33-17b627027548>:4: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-33-17b627027548>:4: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-32-2f28bf6ca683>:5: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/vz/p2k6whv90_91t9567mpw6x1h0000gn/T/tmpr3j1ipyp/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.17252, step = 1\n",
      "INFO:tensorflow:global_step/sec: 0.587756\n",
      "INFO:tensorflow:loss = 0.48899, step = 101 (170.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.595502\n",
      "INFO:tensorflow:loss = 0.389137, step = 201 (167.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.61189\n",
      "INFO:tensorflow:loss = 0.379296, step = 301 (163.437 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 362 into /var/folders/vz/p2k6whv90_91t9567mpw6x1h0000gn/T/tmpr3j1ipyp/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.580711\n",
      "INFO:tensorflow:loss = 0.376044, step = 401 (172.196 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 500 into /var/folders/vz/p2k6whv90_91t9567mpw6x1h0000gn/T/tmpr3j1ipyp/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.375983.\n",
      "WARNING:tensorflow:From <ipython-input-33-17b627027548>:5: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-32-2f28bf6ca683>:5: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/vz/p2k6whv90_91t9567mpw6x1h0000gn/T/tmpr3j1ipyp/model.ckpt-500\n",
      "Accuracy: 0.618045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "classifier = learn.Estimator(model_fn=bag_of_words_model) \n",
    "# Train and predict \n",
    "classifier.fit(X_train, y_train, steps=500) \n",
    "y_predicted = [ p['class'] for p in classifier.predict(X_test, as_iterable=True)] \n",
    "score = accuracy_score(y_test, y_predicted) \n",
    "print('Accuracy: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(features, labels, mode):\n",
    "  \"\"\"RNN model to predict from sequence of words to a class.\"\"\"\n",
    "  # Convert indexes of words into embeddings.\n",
    "  # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "  # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "  # EMBEDDING_SIZE].\n",
    "  word_vectors = tf.contrib.layers.embed_sequence(\n",
    "      features[WORDS_FEATURE], vocab_size=n_words, embed_dim=EMBEDDING_SIZE)\n",
    "\n",
    "  # Split into list of embedding per word, while removing doc length dim.\n",
    "  # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "  word_list = tf.unstack(word_vectors, axis=1)\n",
    "\n",
    "  # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "  cell = tf.contrib.rnn.GRUCell(EMBEDDING_SIZE)\n",
    "\n",
    "  # Create an unrolled Recurrent Neural Networks to length of\n",
    "  # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "  _, encoding = tf.contrib.rnn.static_rnn(cell, word_list, dtype=tf.float32)\n",
    "\n",
    "  # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "  # neural network of last step) and pass it as features for softmax\n",
    "  # classification over output classes.\n",
    "  logits = tf.layers.dense(encoding, MAX_LABEL, activation=None)\n",
    "  return estimator_spec_for_softmax_classification(\n",
    "      logits=logits, labels=labels, mode=mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "lis_ = defaultdict(list)\n",
    "import xlrd\n",
    "cancer_text = pd.read_excel('cancerText.xlsx', sheetname='Sheet2')\n",
    "\n",
    "cancer_text.columns = ['ID','Text']\n",
    "cancer_data = pd.read_csv('train_aa_extra.csv')\n",
    "cancer_data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "frames = [cancer_data, cancer_text]\n",
    "cancerTotal = pd.merge(cancer_data, cancer_text, on='ID', how='inner')\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancerTotal.Text, cancerTotal.Class, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot use a string pattern on a bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-2a2c289a4486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-78-2a2c289a4486>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(unused_argv)\u001b[0m\n\u001b[1;32m     49\u001b[0m       MAX_DOCUMENT_LENGTH)\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mx_transform_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mx_transform_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, unused_y)\u001b[0m\n\u001b[1;32m    166\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_document_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mWord\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mid\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, unused_y)\u001b[0m\n\u001b[1;32m    148\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py\u001b[0m in \u001b[0;36mtokenizer\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     49\u001b[0m   \"\"\"\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32myield\u001b[0m \u001b[0mTOKENIZER_RE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot use a string pattern on a bytes-like object"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 10\n",
    "EMBEDDING_SIZE = 50\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "MAX_LABEL = 15\n",
    "WORDS_FEATURE = 'words'  # Name of the input words feature.\n",
    "\n",
    "\n",
    "def estimator_spec_for_softmax_classification(logits, labels, mode):\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions={'class': predicted_classes,'prob': tf.nn.softmax(logits)})\n",
    "\n",
    "    onehot_labels = tf.one_hot(labels, MAX_LABEL, 1, 0)\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "        eval_metric_ops = {'accuracy': tf.metrics.accuracy(labels=labels, predictions=predicted_classes)}\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(unused_argv):\n",
    "    global n_words\n",
    "\n",
    "\n",
    "  # Process vocabulary\n",
    "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n",
    "      MAX_DOCUMENT_LENGTH)\n",
    "\n",
    "    x_transform_train = vocab_processor.fit_transform(X_train)\n",
    "    x_transform_test = vocab_processor.transform(X_test)\n",
    "\n",
    "    x_train = np.array(list(x_transform_train))\n",
    "    x_test = np.array(list(x_transform_test))\n",
    "\n",
    "    n_words = len(vocab_processor.vocabulary_)\n",
    "    print('Total words: %d' % n_words)\n",
    "\n",
    "  # Build model\n",
    "  # Switch between rnn_model and bag_of_words_model to test different models.\n",
    "    model_fn = rnn_model\n",
    "    if FLAGS.bow_model:\n",
    "    # Subtract 1 because VocabularyProcessor outputs a word-id matrix where word\n",
    "    # ids start from 1 and 0 means 'no word'. But\n",
    "    # categorical_column_with_identity assumes 0-based count and uses -1 for\n",
    "    # missing word.\n",
    "        x_train -= 1\n",
    "        x_test -= 1\n",
    "        model_fn = bag_of_words_model\n",
    "    classifier = tf.estimator.Estimator(model_fn=rnn_model)\n",
    "\n",
    "  # Train.\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(x={WORDS_FEATURE: x_train},\n",
    "        y=y_train,\n",
    "        batch_size=len(x_train),\n",
    "        num_epochs=None,\n",
    "        shuffle=True)\n",
    "    classifier.train(input_fn=train_input_fn, steps=100)\n",
    "\n",
    "  # Predict.\n",
    "    test_input_fn = tf.estimator.inputs.numpy_input_fn(x={WORDS_FEATURE: x_test},\n",
    "      y=y_test,\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "    predictions = classifier.predict(input_fn=test_input_fn)\n",
    "    y_predicted = np.array(list(p['class'] for p in predictions))\n",
    "    y_predicted = y_predicted.reshape(np.array(y_test).shape)\n",
    "\n",
    "  # Score with sklearn.\n",
    "    score = metrics.accuracy_score(y_test, y_predicted)\n",
    "    print('Accuracy (sklearn): {0:f}'.format(score))\n",
    "\n",
    "  # Score with tensorflow.\n",
    "    scores = classifier.evaluate(input_fn=test_input_fn)\n",
    "    print('Accuracy (tensorflow): {0:f}'.format(scores['accuracy']))\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
